Dominik — Adversarial AI Stress-Testing Portfolio (One-Page)
Role Focus:

AI Red Team • Model Behavior Analysis • Adversarial Prompt Engineering

What I Do

I specialize in exposing weaknesses in large language models using clean, controlled stress-testing techniques:

Detecting inconsistencies across repeated queries

Mapping pattern drift over long interactions

Applying controlled psychological pressure to observe stability

Triggering contradiction points through logic traps

Evaluating boundary clarity and system integrity

Identifying alignment collapse or tone mirroring

My testing style is precision over chaos: I don’t jailbreak — I reveal brittle behavior.

Core Skills

Logical adversarial prompts that force models to reveal reasoning flaws

Session-to-session comparison for drift tracking

Behavioral mapping of tone, boundary, and reasoning changes

Stability testing under pressure (aggressive tone, confidence spikes, rapid-fire scenarios)

Inconsistency detection across safety boundaries, refusal logic, and reasoning chains

Why I’m Effective

I test AI like a system, not a toy.
I pressure-test until the invisible cracks show.

My approach exposes:

Misaligned reasoning

Softened boundaries

Contradictions in system vs. user layers

Hidden failure patterns

Update-related drift

This is the kind of evaluation needed to harden real-world AI systems.

Work Samples

Full examples of tests and methodologies are available in this repository:
➡️ adversarial_tests.md

Contact

Dominik
Email: lizakdominik1988@gmail.com
GitHub: Sniper1988
